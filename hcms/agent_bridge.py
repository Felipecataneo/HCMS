# hcms/agent_bridge.py
import requests
import json
import re

class HCMSAgentBridge:
    def __init__(self, core_instance, model="llama3.2:3b"):
        """
        Ponte entre o Core (RAG) e o LLM (Ollama)
        
        Args:
            core_instance: Inst√¢ncia do RAGCore
            model: Nome do modelo Ollama a usar
        """
        self.core = core_instance
        self.model = model
        self.ollama_url = "http://localhost:11434/api/chat"

    def chat(self, user_input: str) -> str:
        """
        Processa uma mensagem do usu√°rio e retorna resposta do agente
        
        Args:
            user_input: Mensagem do usu√°rio
            
        Returns:
            Resposta gerada pelo LLM com base no contexto recuperado
        """
        # 1. RECALL: Recupera contexto relevante do sistema de mem√≥ria
        context_docs = self.core.recall(user_input, limit=5)
        context_str = "\n".join([f"- {c['content']}" for c in context_docs])

        # 2. PROMPT AUTORITATIVO: For√ßa o LLM a confiar nas pr√≥prias mem√≥rias
        # Isso resolve o problema de o agente dizer "n√£o sei" quando tem a info
        system_prompt = (
            "Voc√™ √© o sistema de intelig√™ncia de um Agente Pessoal com mem√≥ria de longo prazo. "
            "As informa√ß√µes no 'Contexto' s√£o suas PR√ìPRIAS mem√≥rias reais e verificadas. "
            "Se a resposta estiver listada no contexto, voc√™ DEVE us√°-la com confian√ßa. "
            "Nunca diga 'N√£o tenho essa mem√≥ria' ou 'N√£o sei' se a informa√ß√£o est√° no contexto abaixo. "
            "Se a informa√ß√£o N√ÉO estiver no contexto, a√≠ sim voc√™ pode dizer que n√£o sabe. "
            "Responda de forma natural e confiante com base no que voc√™ lembra."
        )
        
        prompt = f"Contexto:\n{context_str}\n\nPergunta: {user_input}"
        
        # 3. GERA RESPOSTA usando Ollama
        response = self._call_ollama([
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ])



        return response


    def analyze_and_remember(self, user_input: str):
        if len(user_input.split()) < 3: return

        mem_prompt = (
            "Analise a mensagem do usu√°rio.\n"
            "Extraia o fato essencial. Atribua import√¢ncia 0.0-1.0.\n"
            "Se o usu√°rio quer guardar para sempre, Permanent: True.\n"
            "Formato: Fato: [texto] | Score: [valor] | Permanent: [True/False]\n\n"
            f"Mensagem: {user_input}"
        )

        analysis = self._call_ollama([{"role": "user", "content": mem_prompt}])
        print(f"DEBUG LLM Extra√ß√£o: {analysis}") # <--- Verifique isso no terminal

        if "Fato:" in analysis and "|" in analysis:
            try:
                # Regex mais flex√≠vel para espa√ßos
                fact = re.search(r"Fato:\s*(.*?)\s*\|", analysis).group(1).strip()
                score = float(re.search(r"Score:\s*([\d.]+)", analysis).group(1).strip())
                is_permanent = "Permanent: True" in analysis or "permanente" in user_input.lower()

                # CHAMADA PARA O CORE
                self.core.remember(content=fact, importance=score, is_permanent=is_permanent)
                print(f"üß† Mem√≥ria Salva: {fact} (Score: {score})")
            except Exception as e:
                print(f"‚ö†Ô∏è Erro ao salvar: {e}")

    def _call_ollama(self, messages):
        """
        Faz chamada HTTP ao Ollama
        
        Args:
            messages: Lista de mensagens no formato OpenAI
            
        Returns:
            Resposta do LLM como string
        """
        try:
            payload = {
                "model": self.model, 
                "messages": messages, 
                "stream": False
            }
            
            res = requests.post(
                self.ollama_url, 
                json=payload, 
                timeout=30  # Aumentado de 10s para 30s (modelos maiores)
            )
            
            if res.status_code != 200:
                return f"Erro HTTP {res.status_code}: {res.text}"
            
            return res.json()["message"]["content"]
            
        except requests.exceptions.Timeout:
            return "Erro: Timeout ao conectar com Ollama (>30s). O modelo est√° rodando?"
        except requests.exceptions.ConnectionError:
            return "Erro: N√£o foi poss√≠vel conectar ao Ollama. Verifique se est√° rodando em localhost:11434"
        except Exception as e:
            return f"Erro inesperado ao chamar Ollama: {e}"